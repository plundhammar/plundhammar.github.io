[{"content":"This is a preliminary, hands-on, and maybe useful walk-trough of a sequential neural network in PyTorch. This is a summary of some notes I have been taking while reading about different topics in machine learning, see Refs. [1-3].\nSuppose one has data that consists of an independent vector $\\mathbf{x}$ and a dependent vector $\\mathbf{y}$ such that $\\mathbf{y} = f(\\mathbf{x})$ for some function $f$. Let say we knew the set $(\\mathbf{x}, \\mathbf{y})$ then we could guess what the function $f$ is. If vector $\\mathbf{x}$ contains the entire domain of $f$ then we are done, but if $\\mathbf{x}$ only contains part of the domain then the problem of estimating $f$ becomes more elusive. Let $\\widehat{f}$ denote an estimation of $f$.\nIntroduce the Lost Function $L$, which takes an estimation $\\widehat{f}$ and returns a value corresponding to wether or not the estimate is \u0026lsquo;good\u0026rsquo; or \u0026lsquo;bad\u0026rsquo;. For instance, let $$ L(\\widehat{f}) = \\sum_{i\\in I}\\bigg[\\widehat{f}(x_i)-y_i\\bigg]^2 $$ with $|I|$ beign the number of entries in the vectors $\\mathbf{x}$ and $\\mathbf{y}$, and $(x_i, y_i)\\in (\\mathbf{x},\\mathbf {y})$. Then $L$ will be close to zero if the evaluated values of the estimated function $\\widehat{f}$ is close to the values of the dependent vector $\\mathbf{y}$. If $L(\\widehat{f})=0$ then $\\widehat{f}$ equals $f$ on the resticted domain covered by $\\mathbf{x}$. However, note that this does not imply that $\\widehat{f}$ approximately equals $f$ outside the domain covered by $\\mathbf{x}$.\nIf we knew that $f$ is linear then we might guess that $$ \\widehat{f}(\\mathbf{x}) = a\\mathbf{x} + b, $$ with $a$ and $b$ being matrices ($b$ is often called bias). Given a set $(\\mathbf{x},\\mathbf{y})$ we could then find the matrices that minimizes som lsot function $L$ with $\\widehat{f}$ given by the linear form above. However, when considering a general $f$ then one good approach is to modify the above expression slightly such that more free paramaters are introduced.\nActivation Function Take the expression above and alter it in the following way $$ \\widehat{f}(\\mathbf{x}) = R[af(\\mathbf{x}) + b] $$ with $R$ being a activation function and has the property that it is slightly non-linear. For instance, take $R(x) = x$ for $x\\geq 0$ and otherwise $R(x)=0$, then the estimate $\\widehat{f}$ is made non-negative by the action of $R$. This specific activation function is called the Rectifier function. The output values can always be scaled and altered to be non-negative, so this is an equivalent problem. What this activation function introduces, with it being non-linear, is the possibility to introduce more free paramters. That is, consider the following estimate $$ \\widehat{f}(\\mathbf{x}) = a_2R\\big[ a_1 \\mathbf{x}+b_1\\big]+b_2 = [\\widehat{f}_2\\circ \\widehat{f}_1](\\mathbf{x}) $$ with $\\widehat{f}_i=R[a_i\\mathbf{x}] + b$. Note that if the activation function were linear there would exist $a_3$ and $b_3$ such that $$ [\\widehat{f}_2\\circ \\widehat{f}_1](\\mathbf{x}) = a_3R[\\mathbf{x}] + b_3, $$ not introucing more free parameters. In general one can write $$ \\widehat{f}(\\mathbf{x}) = [\\widehat{f}_N\\circ\\dots\\circ \\widehat{f}_1](\\mathbf{x}) $$ with each function given by $$ \\widehat{f}_i(\\mathbf{x}) = R(a_i\\mathbf{x}) +b_i. $$\nData We will be investigating data consisting of the weight and length of 9 different species of fish found HERE. The first $75~%$ of the row will constitute the training data and the rest the test data. We will use the training data to alter the weighhts of the neural network, then test our resulting model on the test data. In utils.py there are some helpful functions that keeps the cluttering in this document to the minimum. Importing everthing from utils.py we can for the training and test datasets.\nNeural Networks Inspired by the structure of biological neural network a neural network in maachine learning is a model consisting of connected units of nodes. For a given input this model produces an output based on weights that is altered to yield the best output. In our case, we will adapt the structure given by the PyTorch library where the data is loaded into a so-called Dataloader which will be explained later. This simplifies the tracking of all the operations that needs to be made in the model.\nThe question remains how the parameters should be choosen such that the result of our estimet $\\widehat{f}$ as closely as possible resemmbles the unknown function $f$. With our loss function $L$ defined, we know that a small value of $L$ will result in a better estimate. Our goal is therefore to minimize $L$Â· With a sequmential neural network shown above where we choose the estimate to have the form $$ \\widehat{f}(\\mathbf{x}) = [\\widehat{f}_N\\circ\\dots\\circ \\widehat{f}_1](\\mathbf{x}) $$ with $$ \\widehat{f}_i(\\mathbf{x}) = R(a_i\\mathbf{x}) +b_i, $$ we see that the loss function must be a function of the parameters $(a_i)_{nm}$ and $(b_i)_{nm}$ of the matrices $a_i$ and $b_i$. Thus, if we update the parameters such that $$ (a)_{nm} \\to (a)_{nm} - \\partial L/\\partial (a)_{nm} , ~~(b)_{nm} \\to (b)_{nm}- \\partial L/\\partial (b)_{nm} $$ this will converge to the values of $a_i$ and $b_i$ such that $L$ is minimized, given that $L$ is convex with respect to the $a_i$\u0026rsquo;s and $b_i$\u0026rsquo;s.\nIn PyTorch In PyTorch one can inherite many practical attributes from the nn.Module class, as shown below. Here I initilize a matrix of size $2\\times N$ for some $N\\in\\mathbb{Z}$, then a number of \u0026lsquo;\u0026lsquo;middle\u0026rsquo;\u0026rsquo; matrices are created of size $N\\times N$, and lastly a matrix of size $N\\times 9$ is created. The neural network will now, in the forward method do the following\nTake the input $\\mathbf{x}$ and multiply it with matrix $M_1$ of size $2\\times N$ and add a bias $b_1$ (remember $\\mathbf{x}$ is a vector with columns [Weight, Length] of all the different fish species and with 75 % of the number of rows of all the data); Apply the rectifier function to the result $M_1\\mathbf{x} + b_1$; Do step 1) and 2) but for the middle matrices of size $N\\times N$ for some number of times; Multply the result with the matix $M_2$ of size $N\\times 9$ and adding a bias vector $b_2$ yielding a vector of size $9$ corresponding to indeces of the label set. class NeuralNetwork(nn.Module): def __init__(self,num_matrices=3,inner_matrix_size = 100): super().__init__() self.Matrix1 = nn.Linear(2,inner_matrix_size) self.Matrices = [nn.Linear(inner_matrix_size,inner_matrix_size) \\ for i in range(num_matrices)] self.Matrix2 = nn.Linear(inner_matrix_size,9) self.R = nn.ReLU() def forward(self,x): x = x.view(-1,2).float() x = self.R(self.Matrix1(x)) for Matrix in self.Matrices: x = self.R(Matrix(x)) x = self.Matrix2(x) return x Creating an instance f of the neural network will initiate random initial values to all paramters of the values. Also, note that the vector $b_i$ is never mentioned in the NeuralNetwork class, this vector is implied and one has to specifically specify ...,bias=False in the nn.Linear(...) method to not include the bias vector.\nnum_matrices = 1 inner_matrix_size = 2500 f = NeuralNetwork(num_matrices,inner_matrix_size) Here we have created a neural network with one \u0026lsquo;\u0026lsquo;middle\u0026rsquo;\u0026rsquo; matrix of size $2500\\times 2500$. Our neural network therefore contains $2500\\times 2500 + 2500\\times 2 +2+ 2500\\times 9 + 9 = 6277511$ parameters (!).\nPytorch\u0026rsquo;s DataLoader The datasets train_ds and test_ds has the columns [Species, Weight,Length] and the following number of rows\ntest_ds : 1019 train_ds: 3061 We will train the model with the training data in batches of a set size. This means that the adjusting of the parameters of the model is parallelized over the batches, making the training process quicker. Furthermore, the model will be trained several so-called epochs over the same training data. This means that the the output of the first adjustment of the paramters looping through all the training data will be used as input in the next epoch adjusting the parameters once again over the same data. This is most easily done with PyTorch DataLoader class, where the size of each batch can be specified.\ntrain_dl = DataLoader(train_ds, batch_size=20) Training For a chosen loss function $L$ in our case we choose the Cross Entropy Loss, and a way of calculating the gradient, we chose the Stochastic Gradient Decent, the training consists of adjusting the parameters for each batch over all the epochs.\nnum_epochs = 20 opt = SGD(f.parameters(), lr = 0.01) L = nn.CrossEntropyLoss() losses = [] epochs = [] predicted_test =[] predicted_train =[] for epoch in range(num_epochs): print(f\u0026#39;Epoch {epoch}/{num_epochs-1}\u0026#39;) for i, (x,y) in enumerate(train_dl): opt.zero_grad() # flush gradient loss= L(f(x),y) loss.backward() opt.step() losses.append(loss.squeeze().detach().numpy()) epochs.append(epoch + i/len(train_dl)) predicted_test.append(f(test_ds.x).argmax(axis=1).numpy()) predicted_train.append(f(train_ds.x).argmax(axis=1).numpy()) Results Below we see the loss function over all epochs and batches. It is slowly decreasing, meaning that our estimate gets better at recreating the true values of the training data.\nBut what we really care about is how well the model describes data it has not yet seen, i.e. the test_data.\nAbove is for the training data, which we would expect the model to be able to describe (it has already seen it many times!). Of all the training data, the best model where able to label 80.08 % of the fish specis correctly and 19.92 % incorrectly.\nBelow is the amount of correct predictions for the training data (data the model has not yet seen).\nAccually, for the test data the same percentage of correct and incorrect labeling were produced, which seems a bit odd. it kind of seems that the model is not able to attain the data characteristics fully and sort of asymptotes to some best version of this specific model. Looking closer at what species the model has trubble labeling below,\nwe see that the model never really gets the labeling of Otolithoides biauritus and Setipinna taty right. Since they account for abut $21~%$ of the test_data it makes sense why the amount of correct predictions never really gets higher than about $79~%$. However, it is not yet clear why the model has so much trubble predicting the species name of these specific species. Well, looking at the data for these two species,\nwe see that they are very similar in weight and length and given a velua of the weight and length, it is basically 50/50 whihc label between the two the model will choose.\nWe can test to remove one of them and retrain the data on the subset of data containg all but the species Otolithoides biauritus. That is , we consider the following data set.\nTraining with this data set for the same parameters we get the loss function behaviour below.\nAnd the amount of correct guesses is accually a bit better.\nOf the total number of labels of the test data, the best model were able to label 88.25 % correctly and just 11.75 % incorrectly.\nConclusions We have here briefly describe the logic behind sequential neural networks. The logic is quite easy to follow, but precisly how the system paramters translate to accual weights and lengths of different fish specimen are still a mystery. Looking at all these results I would also have expected much better performance of the model when only considering the data set in Fig. [X] considering that the data clouds are quite distinct from each other. In this specific task even I would perform better than the model. I have not specifically explained how the Cross Entropy Loss function works (or even how it is defined) and not what the Stochastic Gradient Decent is. But I will leve this here and possibly go into further details in the future.\nReferences Neural Networks and Deep Learning, Nielsen, Michael A. Understanding Machine Learning: From Theory to Algorithms, Shalev-Shwartz, Shai and Ben-David Shai PyTorch Course (2022), Part 1-3, Mr P Solver Link ","permalink":"http://localhost:1313/posts/sequential_networks/","summary":"\u003cp\u003e\u003cstrong\u003eThis is a preliminary, hands-on, and maybe useful walk-trough of a sequential neural network in PyTorch. This is a summary of some notes I have been taking while reading about different topics in machine learning, see Refs. [1-3].\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSuppose one has data that consists of an independent vector $\\mathbf{x}$ and a dependent vector $\\mathbf{y}$ such that $\\mathbf{y} = f(\\mathbf{x})$ for some function $f$. Let say we knew the set $(\\mathbf{x}, \\mathbf{y})$ then we could guess what the function $f$ is. If vector $\\mathbf{x}$ contains the entire domain of $f$ then we are done, but if $\\mathbf{x}$ only contains part of the domain then the problem of estimating $f$ becomes more elusive. Let $\\widehat{f}$ denote an estimation of $f$.\u003c/p\u003e","title":"Sequential Neural Networks"},{"content":"In this post I want to try to reconstruct an image from the simulated data in the previous post. Supposedly the original source consists of two point sources close together around z= -0.5 cm. However, I found it hard to understand the properties of the reconstruction. I will need to redo this simulation with full a priori information about the simulated source.\nSo, I have looked into the reconstruction application Compton_MLEM by Matt Leigh, Ref. [1]. It is written in CUDA Ref. [2], an extension of the C programming language with the ability to specify thread-level parallelism in C. This programming language was new to me, and I had a hard time simply trying to install the necessary software to make everything work (especially the nividia drivers caused me some headache)! I recommend doing the tutorial in Ref. [3] just to get a feeling for the language, and to demystify it. It is very similar to C++, but with some additional methods to parallelize the calculations.\nThe CUDA-program Compton_MLEM reconstructs an image from list-mode data using the Maximum-Likelihood Expectation Maximization Algorithm; an overview of the theory can be found in one of my previous posts.\nNote: The Compton_MLEM does not work with the newest version of CUDA. It seems that the flag sm_30 was deprecated when building the executable, see Ref. [4]. However, simply removing it in makefile works, or at least does not return any errors. That is, in makefile there is one value specifying the flags of the build NVFLAGS = -arch=sm_30 '-std=c++11' which I changed to NVFLAGS = '-std=c++11' and it seems to work.\nIn the repo of Compton_MLEM it seems that the folder /MLEM_NO_MATRIX is both the fastest and the best maintained part of this project. In it there are three main CUDA scripts found in the /Source folder:\nCPUFunctions.cu GraphicsCardFunctions.cu and Parallel_Reform.cu The Parallel_Reform.cu is the main script and allocates memory and jobs between the CPU and GPU through the scripts GraphicCardFunctions.cs and CPUFunctions.cu. I will come back to the functionality of these scripts in the future, there exists some questions regarding the calculation of some values. Using the data in the I tried to reconstruct the source distribution in the cube $X\\times Y\\times Z = [-10,10]\\times [-10,10]\\times [-10,10]~\\mathrm{mm}^3$ in figure 1 and 2. The initial image is the back projected source distribution. It looks kind of strange. I would have guessed that the backprojection would have some symmetry properties along some axis. However, the 150th iteration looks less noisy and more closely like one (two?) point sources.\n(a): The backprojected image initiating the algorithm. (b): The 150th iteration. Figure 1: Slices of the $z$-axis.\n(a): The backprojected image initiating the algorithm. (b): The 150th iteration. Figure 2: Slice of the $z$-axis.\nSince I don\u0026rsquo;t know exactly where the source is placed I can only guess. Looking at figure 1b it seems that we have overestimated the $X$ and $Y$ intervals, perhaps even the $Z$ interval. In figure 3 and 4 I consider the voxel space in $X\\times Y\\times Z = [1.0,1.0]\\times [1.0,1.0]\\times [5.0,5.0]~\\mathrm{mm}^3$.\n(a): The backprojected image initiating the algorithm. (b): The 150th iteration. Figure 3: Zoomed in version of figure 1.\n(a): The backprojected image initiating the algorithm. (b): The 150th iteration. Figure 4: Zoomed in version of figure 2.\nNow it kind of looks like a circle emerges, with no real symmetric properties. I may have misunderstood the algorithm but these results seem strange to me.\nConclusion The Compton_MLEM program returns an image. What exactly I\u0026rsquo;m looking at requires me to understand the CUDA script in depth. I have some queries on how the Klein-Nishina values are calculated but I can\u0026rsquo;t really draw any conclusions as of yet. Furthermore, I do not know exactly where the source is positioned in the list-mode data I have (though this might be more realistic). I will redo this simulation study with another data set where the system parameters are known, and try to catch the logic in the code.\nReferences [1] Compton_MLEM\n[2] CUDA\n[3] https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/\n[4] https://forums.developer.nvidia.com/t/ptxas-fatal-value-sm-30-is-not-defined-for-option-gpu-name/163708\n","permalink":"http://localhost:1313/posts/compton_mlem-a-gpu-based-image-reconstruction-application-in-cuda/","summary":"\u003cp\u003e\u003cstrong\u003eIn this post I want to try to reconstruct an image from the simulated data in the previous post. Supposedly the original source consists of two point sources close together around z= -0.5 cm. However, I found it hard to understand the properties of the reconstruction. I will need to redo this simulation with full a priori information about the simulated source.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSo, I have looked into the reconstruction application \u003ccode\u003eCompton_MLEM\u003c/code\u003e by Matt Leigh, Ref. [1]. It is written in CUDA Ref. [2], an extension of the C programming language with the ability to specify thread-level parallelism in C. This programming language was new to me, and I had a hard time simply trying to install the necessary software to make everything work (especially the nividia drivers caused me some headache)! I recommend doing the tutorial in Ref. [3] just to get a feeling for the language, and to demystify it. It is very similar to C++, but with some additional methods to parallelize the calculations.\u003c/p\u003e","title":"`Compton_MLEM`, A GPU-Based Image Reconstruction Application in CUDA"},{"content":"The energies of a Compton scatter chain are correlated through the Compton scattering equation as well as the Klein-Nishina distribution. This means that some vital statistics must exist, and perhaps be possible to calculate in closed expression. In this post I wanted to give it a try and compare it to some simulated data.\nI recently got a hold on some simulated list-mode data of photons interacting with a silicon volume. That is, a list $$ [\\mathbf{r}_1,E_1,\\mathbf{r}_2,E_2] $$\nof\ninteraction position $\\mathbf{r}_1$ of Compton event with the deposited energy $E_1$ of the scattered electron and the interaction position $\\mathbf{r}_2$ of either a photoabsorption or another Compton scatter event following the first scattering event, with the corresponding deposited energy $E_2$ of the scattered electron. Something that occurred to me was that basically all of the statistics are known in this problem. That is, for a scatter series as in figure 1 we have that Ref. [1] $$ E_\\gamma\u0026rsquo; = \\frac{E_\\gamma}{1+\\varepsilon(1-\\cos\\theta)} $$\nwith $\\theta$ being the scatter angle and $\\varepsilon = E_\\gamma\u0026rsquo;/E_\\gamma$, and canonical for $E_\\gamma\u0026rsquo;\u0026rsquo;$ with $\\theta\u0026rsquo;$ and $\\varepsilon\u0026rsquo;$ defined similarly to above.\nFigure 1: A serie of two scatter events.\nSince the scatter angle is inherently stochastic with a probability density function given by a function $\\mathrm{pr}(\\cos\\theta)$ proportional to the Klein-Nishina distribution, Ref. [2], i.e. $$ \\mathrm{pr}(\\cos\\theta)\\propto \\frac{1}{1+\\varepsilon(1-\\cos\\omega)}\\left(\\frac{1}{1+\\varepsilon(1-\\cos\\omega)}+1+\\varepsilon(1-\\cos\\omega-(1-\\cos^2\\omega))\\right). $$ That means that we can calculate the moments of $E_\\gamma\u0026rsquo;$ and $E_\\gamma\u0026rsquo;\u0026rsquo;$. For instance, $$ \\mathbb{E}\\left[E_\\gamma\u0026rsquo;\\right] = E_\\gamma\\int_{-1}^1\\mathrm{d}(\\cos\\theta)\\frac{\\mathrm{pr}(\\cos\\theta)}{1+\\varepsilon(1-\\cos\\theta)}. $$ For $\\varepsilon = 1$, i.e. $E_\\gamma = 511~\\mathrm{keV}$, we can calculate the expectation value of the deposited energy $E_1$: $$ \\mathbb{E}\\left[E_1\\right] = 511\\times\\left[\\frac{\\log(3)-\\frac{28}{81}}{\\frac{40}{9}-\\log(3)}\\right]~\\mathrm{keV} \\approx 511\\times 0.656\\mathrm{keV}\\approx 335\\mathrm{keV} $$ For a given $E_\\gamma\u0026rsquo;$ we can calculate the expectation value and standard deviation of $E_\\gamma\u0026rsquo;\u0026rsquo;$ in the same manner as above. I got some data simulating the setup in figure (2).\nFigure 2: Geometry of the simulation.\nThe events are the positions and energies as described above, and illustrated in figure (3)\nFigure 3: The events in the silicon solid.]]\nIn figure (4) the simulated data is shown together with the expectation value (red line) and one standard deviation away from the expectation value (blue lines) as well as the line where photo absorption happens (gray).\nFigure 4: Energy distribution of some simulated list-mode data and its statistics.]]\nNote that the realizations get denser at $E_1 = 335~\\mathrm{keV}$, as calculated above.\nConclusion The result in figure 4 looks kind of right. However, I implicitly assume that $E_1$ is given when calculating the expectation value of $E_2$. This is of course true, since the events are causal. What could have been taken into consideration is the distribution of $E_1$ and subsequently how the pair $(E_1,E_2)$ is distributed in the energy space. I could have gone further in the statistics, and maybe I will in the future, but I will leave it like this for now.\nReferences [1] A. H. Compton 1923 A Quantum Theory of The Scattering of X-Rays by Light Elements Phys. Review 21.5 483-502, Link\n[2] Klein O and Nishina Y 1929 Uber die Streuung von Strahlung durch freie Elektronen nach der neuen relativistischen Quantendynamik von Dirac Z Phys. 52 853â69, Link\n","permalink":"http://localhost:1313/posts/properties-of-list-mode-data-of-a-compton-camera/","summary":"\u003cp\u003e\u003cstrong\u003eThe energies of a Compton scatter chain are correlated through the Compton scattering equation as well as the Klein-Nishina distribution. This means that some vital statistics must exist, and perhaps be possible to calculate in closed expression. In this post I wanted to give it a try and compare it to some simulated data.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI recently got a hold on some simulated list-mode data of photons interacting with a silicon volume. That is, a list\n$$\n[\\mathbf{r}_1,E_1,\\mathbf{r}_2,E_2]\n$$\u003c/p\u003e","title":"Properties of List-Mode data from a Compton Camera"},{"content":"The List-Mode Maximum Likelihood Expectation Maximization (LM-MLEM) algorithm is widely used in Compton camera reconstruction. It\u0026rsquo;s well suited for the list-mode data architecture and easy to parallelize. This post will be somewhat technical and a sort of summary of the main arguments found in the litterature.\nThe problem has been stated many times, Ref.[1-8]. I will state it as follows (I will use the same notation as in Ref.[8]), and the proof of it will be the subject of a future post:\nLet $\\lambda$ be the mean and variance of the Poisson distribution of some source. The following sequence converges to $\\lambda$: $$ \\widehat{\\lambda}_j^{l+1} = \\frac{\\widehat{\\lambda}_j^{(l)}}{s_j}\\sum_{i=1}^N t_{ij}\\frac{1}{p_i^{(l)}}~~\\text{with}~~p_i^{(l)} = \\sum_{k=1}^M t_{ik}\\widehat{\\lambda}^{(l)}_k $$ where\n$t_{ij}$ are elements of the system matrix, indexed on the events $i$ and voxels $j$; $s_j=\\sum_i t_{ij}$is the sensitivity correction. The initialization step consists in calculating $\\widehat{\\lambda}^{(0)}$, generally as the back-projection in the image volume of the $N$ events. The question is now what the system matrix is. Probabilistic Model of the System Matrix Let $x$ be the vector of coordinates of a point $M$ from the source. The probability of occurrence of the measured event $y$ given that $\\mathfrak{p}$ was emitted at $x$ is given by\n$$ p_{Y:X}(y:x,E_0) = \\int p_{Y:\\tilde{Y}}(y:\\tilde{y},E_0)p_{\\tilde{Y}:X}(\\tilde{y}:x,E_0)\\textrm{d}\\tilde{y}. $$\nThis is a trick found in statistical physics, and it can be seen as a variant of the ChapmanâKolmogorov equation Ref. [9]. We will now argue for what properties the two probability distributions $p_{Y:\\tilde{Y}}(y:\\tilde{y},E_0)$ and $p_{\\tilde{Y}:X}(\\tilde{y}:x,E_0)$ should have.\nConsidering $p_{Y:\\tilde{Y}}(y:\\tilde{y},E_0)$, this distribution could be summarized to have the task of answering the following question\nHow likely is it that the measured event $y$ was registered given that the real event $\\tilde{y}$ happened? Since the measurement is based on the measurement and real event of the position in the first and second scatterer, as well as the measurement and real event of the scattering angle which we may assume to be independent we have\n$$ p_{Y:\\tilde{Y}}(y:\\tilde{y},E_0) =p_{V_1:\\tilde{V}_1}(V_1:\\tilde{V_1},E_0)p_{V_2:\\tilde{V}_2}(V_2:\\tilde{V}_2,E_0)p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta},E_0). $$\nThe terms $p_{V_i:\\tilde{V}_i}(V_i:\\tilde{V_i},E_0)$ depends on the spatial resolution of the detector and something that we may impose depending on the detector design. The term $p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta},E_0)$ depends on the energy resolution of the detectors, since the measured scattering angle $\\beta$ is calculated through an energy dependent relation (Compton scattering with additional corrections perhaps).Â Now considering $p_{\\tilde{Y}:X}(\\tilde{y}:x,E_0)$, it is the product of several probabilities:\nThe probability of the Compton scattering process (including Doppler-broadening);\nAbsorption probabilities (in the different environments such as the object, air, absorption of the scattered photon in the scatter detector);\nThe absorption probability of the photon in the absorber;\nThe solid angle subtended at the origin of the particle $M$ by the detector element containing the first hit, which should be considered in 3D since the emission is isotropic, and on the solid angle subtended at the position of the first hit by the detector element where the second hit takes place, which should be considered in 2D since it is related to a cone surface uncertainty through the scattering angle.\nThe probability of the Compton scattering process is proportional to the Compton scattering cross-section given by the Klein-Nishina distribution, denoted $K(\\tilde{\\beta}:E_0)$ at energy $E_0$.\nIn general a solid angle considered in 3D may be written as $A/r^2$ where $A$ is the sector of the sphere of radius $r$ that the half-angle covers. Similarly, in 2D it may be written $A/r$ with the same definition. So, the solid angle subtended at the origin of the particle at $M$ can be written\n$$ \\frac{1}{\\tilde{V}_1M^2}\\int_{S_{\\tilde{V}_1M}}\\textrm{d}S\\propto \\frac{1}{\\tilde{V}_1M^2}\\int_{-\\theta_{\\tilde{V}_1M^2}}^{\\theta_{\\tilde{V}_1M^2}}\\sin(\\theta)\\textrm{d}\\theta \\propto \\frac{\\cos(\\theta_{\\tilde{V}_1M})}{\\tilde{V}_1M^2}. $$\nSimilarly, the solid angle subtended at the position of the first hit by the detector element where the second hit takes place is proportional to $$ \\frac{\\cos(\\theta_{\\tilde{V}_1\\tilde{V}_2})}{\\tilde{V}_1\\tilde{V}_2}, $$ since we now consider it in 2D. Ignoring absorption probabilities (see Ref. [10,11] for a discussion of this), we can now write\n$$ p_{\\tilde{Y}:X}(\\tilde{y}:x,E_0)\\propto K(\\tilde{\\beta}:E_0)\\frac{\\cos(\\theta_{\\tilde{V}_1M})}{\\tilde{V}_1M^2}\\frac{\\cos(\\theta_{\\tilde{V}_1\\tilde{V}_2})}{\\tilde{V}_1\\tilde{V}_2} $$\nIf the real positions are known (there in reality unobservable) $\\tilde{\\beta}$ can be calculated geometrically, let $\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}$ denote the geometrically obtained scattering angle. This may be estimated by measured values of the deposed energies and incident energy of the incident photon by $$ \\cos(\\beta) = 1 - \\frac{m_e c^2E_1}{E_0(E_0-E_1)} $$ with $m_ec^2$ begin the mass energy of the electron. Inserting these probabilities and integrating results in\n$$ p_{Y:X}(y:x,E_0) = \\int p_{Y:\\tilde{Y}}(y:\\tilde{y},E_0)p_{\\tilde{Y}:X}(\\tilde{y}:x,E_0)\\textrm{d}\\tilde{y} $$ $$ =\\int_{\\mathbb{R}^3}\\frac{\\cos(\\theta_{\\tilde{V}_1M})}{\\tilde{V}_1M^2}p_{V_1:\\tilde{V}_1}(V_1:\\tilde{V_1},E_0) $$ $$ \\times\\int_{\\mathbb{R}^3}\\frac{\\cos(\\theta_{\\tilde{V}_1\\tilde{V}_2})}{\\tilde{V}_1\\tilde{V}_2}p_{V_2:\\tilde{V}_2}(V_2:\\tilde{V}_2,E_0) $$ $$ \\times K(\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}:E_0)p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0)\\textrm{d}\\tilde{V}_2\\textrm{d}\\tilde{V}_1 $$\nWe will assume that the uncertainty on the direction of the Compton cone axis may be considered as negligible compared to the uncertainty of the measured Compton scattering angle. Consequently, the real and measured points where $\\mathfrak{p}$ hits the detector may be merged, leading to the simplified formulation: $$ p_{Y:X}(y:x,E_0) =\\frac{\\cos(\\theta_{{V}_1M})}{{V}_1M^2}\\frac{\\cos(\\theta_{{V}_1{V}_2})}{{V}_1{V}_2}K(\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}:E_0)p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0) $$\nThe element $t_{ij}$ of the system matrix $T$ is defined as:\nthe probability of observing the physical event $y_i$ when a photon is emitted by the voxel $v_j$. With our notation this may be written\n$$ t_{ij} =\\int_{v_j} p_{Y_i:X}(y_i:x,E_0)p(x:M\\in v_j)\\textrm{d}x $$\n$$ =\\frac{1}{\\textrm{vol}(v_j)}\\int_{v_j} p_{Y_i:X}(y_i:x,E_0)\\textrm{d}x $$\nwhere $p(x:M\\in v_j)$ is the probability that the physical event took place in $v_j$, and assuming that a voxel is sufficiently small to allow a constant probability inside the voxel. With the expression for $p_{Y_i:X}(y_i:x,E_0)$ we get\n$$ t_{ij}= \\frac{1}{\\textrm{vol}(v_j)}\\frac{\\cos(\\theta_{{V}_1{V}_2})}{{V}_1{V}_2}\\int_{v_j}Â \\frac{\\cos(\\theta_{{V}_1M})}{{V}_1M^2}K(\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}:E_0)p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0)\\textrm{d}x $$\nThe only remaining thing is to model the probability $p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0)$, which describes the uncertainty of the measurement of the scattering angle, $\\beta$. For an ideal detector this probability would be\n$$ p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0) = \\delta(\\beta-\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}). $$\nFor a finite resolution detector, a convenient choice may be the Gaussian distribution with mean $\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}$ and standard deviation $\\sigma_{\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}}$,\n$$ p_{\\beta:\\tilde{\\beta}}(\\beta:\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M},E_0) = \\frac{1}{\\sigma_{\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}} \\sqrt{2\\pi}}\\exp\\left[\\frac{(\\beta-\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M})^2}{2\\sigma^2_{\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}}}\\right]. $$\nThe unknown mean $\\tilde{\\beta}_{\\tilde{V}_1,\\tilde{V}_2,M}$ may be replaced by the measured value $\\beta$. The standard deviation can be calculated frm the knwn detector uncertainties $\\sigma_{\\tilde{E}_1}$ and $\\sigma_{\\tilde{E}_2}$ as:\n$$ \\sigma_{\\tilde{\\beta}} = \\frac{m_ec^2}{E_0^2\\sin(\\tilde{\\beta})}\\sqrt{\\sigma_{\\tilde{E}_1}^2+\\frac{\\tilde{E}_1^2(\\tilde{E}_2+E_0)^2}{\\tilde{E}_2^4}\\sigma_{\\tilde{E}_2}^2} $$\nIt may be seen from the iterative equation above that the factor multiplying the integral in the equation for the system matrix element gets canceled in the iterative reconstruction process and may thus be ignored.\nEven though the sensitivity matrix is given by the system matrix it is often approximated to something easier to calculate since the proper definition of the sensitivity matrix is usually a bottleneck in computation.\nConclusions The LM-MLEM algorithm seems to be fruitful when considering the reconstruction problem of Compton cameras. Understanding the theory is an essential first step to understanding the practicalities of this algorithm. It is the case, as I have experienced, that when implemented naively, this algorithm is very computationally heavy. However, there are smart ways of going about it and the LM-MLEM algorithm have certain properties that makes it very efficient in the right framework.\nReferences Feng, et al. 3-D Reconstruction Benchmark of a Compton Camera Against a Parallel-Hole Gamma Camera on Ideal Data. IEEE Transactions on Radiation and Plasma Medical Sciences\nLehner, et al. 4pi Compton Imaging Using 3-D Position-Sensitive CdZnTe Detector Via Weighted List-Mode Maximum Likelihood. IEEE Transactions on Nuclear Science. Link\nLange, et al. EM Reconstruction Algorithms for Emission and Transmission Tomography.\nWilderman, et al.Improved Modeling of System Response in List Mode EM Reconstruction of Compton Scatter Camera Images. IEEE Transactions on Nuclear Science Link\nCucci et al. List-mode MLEM Image Reconstruction from 3D ML Position Estimates. IEEE Nuclear Science Symposuim \u0026amp; Medical Imaging Conference.Â Link\nWilderman et al. List-Mode Maximum Likelihood Reconstruction of Compton Scatter Camera Images in Nuclear Medicine. 1998 IEEE Nuclear Science Symposium Conference Record. 1998 IEEE Nuclear Science Symposium and Medical Imaging Conference (Cat. No.98CH36255). Link\nCaucci, et al. Maximum Likelihood Event Estimation and List-mode Image Reconstruction on GPU Hardware.IEEE Nuclear Science Symposuim \u0026amp; Medical Imaging Conference. Link\nVoichita Maxim, et al. Probabilistic models and numerical calculation of system matrix and sensitivity in list-mode MLEM 3D reconstruction of Compton camera images Phys.Med.Biol. 61 243. Link\nChapman-Kolmogorov Equation\nWilderman, et al. List-mode maximum likelihood reconstruction of Compton scatter camera images in nuclear medicine Nuclear Science Symp. Conf. Rec. 3 1716â20\nParra. Reconstruction of cone-beam projections from Compton scattered data. IEEE Transactions on Nuclear Science. 47.4. 1543-1550 Link\n","permalink":"http://localhost:1313/posts/understanding-the-system-matrix-and-the-lm-mlem-algorithm/","summary":"\u003cp\u003e\u003cstrong\u003eThe List-Mode Maximum Likelihood Expectation Maximization (LM-MLEM) algorithm is widely used in Compton camera reconstruction. It\u0026rsquo;s well suited for the list-mode data architecture and easy to parallelize. This post will be somewhat technical and a sort of summary of the main arguments found in the litterature.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe problem has been stated many times, Ref.[1-8]. I will state it as follows (I will use the same notation as in Ref.[8]), and the proof of it will be the subject of a future post:\u003c/p\u003e","title":"Understanding the System Matrix and the LM-MLEM Algorithm"},{"content":"Delving into the practicalities of the inverse problem of reconstructing Compton camera images will lead you to Legandre polynomials or Maximum Likelihood methods, most likely.\nSince last week I have been interested in both the analytical reconstruction techniques as well as some of the iterative methods.\nRegarding iterative methods, what I found is that it is difficult to get a feeling on how the iterative methods properly work simply because the algorithms (I write mine in Python) is increadibly slow. However, I have made some progress and have been able to reproduce some results but in a very corse voxel space. One way of simplifying things is to simplify the system matrix. I was reading Ref. [1] where they sort of reduce the forward problem to the following statement:\nThe probability of observing a given measurement $$ A_i = [E_0,E\u0026rsquo;,r_{01},r_{12}] $$ given a gamma ray incident from pixel $j$ is $$ t_{ij} = \\exp(-\\sigma_t(E_0)r_{01})\\frac{\\mathrm{d}\\sigma_C}{\\mathrm{d}\\Omega}\\exp(-\\sigma_t(E\u0026rsquo;)r_{12}), $$ where $\\sigma_t(E)$ is the total absorbtion cross section at energy $E$, $E_0$ and $E\u0026rsquo;$ the initial and scattered gamma-ray energies, respectively, $r_{01}$ the attenuation distance between the first and second interactions, and $\\mathrm{d}\\sigma_C/\\mathrm{d}\\Omega$ the Compton corss section divided by $r_{12}^2$.\nBasically you could write a function calculating the values $\\sigma_t(E)$ and $\\mathrm{d}\\sigma_C/\\mathrm{d}\\Omega$ and the distances on the fly without storing the values in huge matrices. I think this approach could be fruitful.\nThe computational complexity of the list-mode likelihood methods are a huge issue still. However, being clever like in Refs. [2] \u0026amp; [3], one can speed up these types of algorithms by a factor of 250(!) by running it on a 8 GPU units.\nAn interesting counterpart to the iterative modeling is the analytical methods. I read Ref. [4], where they derive all nessecery properties , like the point spread function and deconvolution kernel, to be able to fully (at least in theory disregarding numerical issues) reconstruct any image obtained from Compton camera event data. They start with the following statement:\nThe probability distribution $p(\\omega)$ of measuring an event with scatter angle $\\omega$ is proportional to the differential cross-section, $h(\\cos\\omega)$.\nNote that we are ignoring spatial and energy resolution of the detector as well as absorption probabilities within the detector. This model is a perfect study to understand the mathematical issues arising from the geometry of the Compton camera. They conclude, like in Ref. [5], that simply applying a spherical deconvolution will not work. Specifically, consider the model $$ f(\\Omega_2) = \\int\\mathrm{d}\\Omega_1 g(\\Omega_1)h(\\cos\\omega) $$ where $f(\\Omega_2)$ is the measured image intesity summed over all measured scatter angles and using the statement above. One might be tempted to try to invert this equation directly, and it is accually possible! The show in the Appendix A that there exists an $h^{-1}(\\cos\\omega)$ such that $$ g(\\Omega_1) = \\int \\mathrm{d}\\Omega_2 f(\\Omega_2)h^{-1}(\\cos\\omega) $$ where $$ h^{-1}(\\cos\\omega) = \\sum_{n=0}^\\infty \\left(\\frac{2n+1}{4\\pi}\\right)\\frac{P_n(\\cos\\omega)}{H_n} $$ and $$ H_n = \\frac{2n+1}{2}\\int\\mathrm{d}(\\cos\\omega)h(\\cos\\omega)P_n(\\cos\\omega) $$ and the basis function $P_n(\\cos\\omega)$ are Legandre polynomials. However, this solution is not stable since the coefficient $H_n$ approach zero, and since they are in the denominator in the expression for $h^{-1}(\\cos\\omega)$ this function is not stable. What they do is to define the summation image $g\u0026rsquo;(\\Omega\u0026rsquo;_1)$ which they prove can be expressed by an angular convolution of the line projections with an appropriate point spread function. Then one can reconstruct $g(\\Omega_1)$ from $g\u0026rsquo;(\\Omega_1\u0026rsquo;)$. The point spread function, called $h_{bp}(\\cos\\omega)$ is then derived and the result is $$ h_{bp}(\\cos\\omega) = \\frac{1}{\\sqrt{1-\\cos^2\\omega/2}}\\int_{-\\cos\\omega/2}^{\\cos\\omega/2}\\mathrm{d}z\\frac{h(z)}{\\sqrt{\\cos^2\\omega/2-z^2}}, $$ with $$ h(\\cos\\omega) = h_c(\\cos\\omega)\\frac{1+\\cos^2\\omega+\\frac{\\gamma^2(1-\\cos\\omega)^2}{1+\\gamma(1-\\cos\\omega)}}{(1+\\gamma(1-\\cos\\omega))^2} $$ and $h_c(\\cos\\omega)$ being the Klein-Nishima cross-section, see figure 1. The angle $\\omega$ in the expression above is the angle from a $z$-axis between the axis of the source point and the axis of the cone.\nFigure 1: $h_c(\\cos\\omega)$\nThe differential cross section convolution kernel $h(\\cos\\omega)$ can be seen in figure 2.\nFigure 2: $h(\\cos\\omega)$\nThey said that the integral above has an analytic solution but when I triend to calculate it in scipy it did not result in anything. I did manage to replicate the resulting point spread function, see figure 3.\nFigure 3: $h_bp(\\cos\\omega)$\nIt would be interesting to expand this in Legandre polynomials and deconvolve the measured image intesity summed over scattering angles and compare with an iterative method like LM-MLEM. I suppose the system matrix would take the simple form above shown in Ref. [1], if I manage to write efficient code.\nConclusion Iterative methods are probably the best but they are very computationaly heavy. For a toy model, one could try to simplify the system matrix to just a few calculation for each element.\nAnalytical methods are simple in there assumptions but can easily be expanded upon. Aslo, they are fast and efficient. It seems that the best approach is to expand some transformed version of the point spread function in Legandre polynomials, such that the expansion coefficients are far from zero. Then deconvolve the source responce with the expression of the inverse transformed point spread function.\nReferences [1] 4$\\pi$ Compton Imaging Using 3-D Position-Sensitive CdZnTe Detector Via Weighted List-Mode Maximum Likelihood, Lehner, C.E; He, Zhong; Zhang, Feng Link\n[2]Maximum Likelihood Event Estimation and List-mode Image Reconstruction on GPU Hardware, Caucci, Luca; Furenlid, Lars R.; Barrett, Harrison H. Link\n[3] List-mode MLEM Image Reconstruction from 3D ML Position Estimates, Caucci, Luca; Furenlid, Lars R.; Barrett, Harrison H. Link\n[4] Reconstruction of cone-beam projections from Compton scattered data, Parra L.C Link\n[5] Application of spherical harmonics to image reconstruction for the Compton camera, Basko, Roman; Zeng, Gengsheng L; Gullberg, Grant T Link\n","permalink":"http://localhost:1313/posts/deconvolution-and-system-matrices/","summary":"\u003cp\u003e\u003cstrong\u003eDelving into the practicalities of the inverse problem of reconstructing Compton camera images will lead you to Legandre polynomials or Maximum Likelihood methods, most likely.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSince last week I have been interested in both the analytical reconstruction techniques as well as some of the iterative methods.\u003c/p\u003e\n\u003cp\u003eRegarding iterative methods, what I found is that it is difficult to get a feeling on how the iterative methods properly work simply because the algorithms (I write mine in Python) is increadibly slow. However, I have made some progress and have been able to reproduce some results but in a very corse voxel space. One way of simplifying things is to simplify the system matrix. I was reading Ref. [1] where they sort of reduce the forward problem to the following statement:\u003c/p\u003e","title":"Deconvolution and the System Matrix"},{"content":"One of the first properties encountered in Compton camera image reconstruction is the cone-surface projection. Understandig the properties of this geometrical object will set the ground works for understanding more advanced models.\nI was reading some articles of the early versions of the Compton camera image reconstruction procedure when I came across Ref.[1] talking about the cone-surface projections. They investigated a primitive version of a \u0026lsquo;\u0026lsquo;Compton camera\u0026rsquo;\u0026rsquo;. Well, it kind of had some properties like a Compton camera like a scatterer and an absorber but they assumed that photons only scatter at a fixed given angle. This camera measures the cone-surface projection of a source. What this means is that the intensity measured at a given detector is proportional to the integral over the intersection of the cone and the source, given that the source is homogeneous.Â Specifically, let $f(\\mathbf{x})$ denote the source distribution, $\\mathbf{k}$ the cone axis making angle $\\beta$ with $\\mathbf{n}$ along the cone surface, they defined\n$$ p(\\mathbf{n}) = \\int_0^\\infty f(O+\\mathbf{n}r)r\\mathrm{d}r~~~~~~q_k(\\beta) = \\int_{S(\\mathbf{k},\\beta)}p(\\mathbf{n})\\mathrm{d}s. $$ Here $S(\\mathbf{k},\\beta)$ denotes the circle created by intersecting the cone with axis $\\mathbf{k}$ and half-angle $\\beta$ with the unit sphere, and $O$ the apex of the cone, describing the first interaction point with the scatterer. The quantity $q_{k}(\\beta)$ is the cone-surface projection. It is reasonable that the cone-surface projections are at least proportional to the photon fluxÂ at the detector for a given scattering angle and cone axis.\nNow, they showed a method to invert these equations to obtain the original source $f(\\mathbf{x})$ via Legendre transforms. However, they also showed a figure in which these cone-surface projections were shown (figure 1). I wanted to try to recreate this figure.\nFigure 1: The first two steps of image reconstruction. (a) Plane projections calculated directly from the phantom (b) Cone-surface projections (c) Plane projections evaluated by the algorithm from cone-surface integrals.\nThe cone-surface projections in (b) are calculated for scattering angle $\\beta = \\pi/4$, but they do not entail what cone axis is used or how it is chosen or how the sphere phantom is placed in the space.\nProblem Statement I wanted to generate figure 1 (b), and what I can gather from the article we have:\nA spherical phantom with a radius of 10 mm offset some distance from the rotation axis (not given but looks like around 20 mm from figure 1 (a)); Since they do not talk about specific detector elements I assume that they use all voxels along an axis in the plane perpendicular to the rotation axis. I will however define a set of detector elements along this axis; The cone-surface projection is then the total contribution of a cone with half-angle $\\beta = \\pi/4$ with some axis $\\mathbf{k}$ (not explicitly given but I will take the one perpendicular to the rotation axis) with the spherical source. Practical Issues and Result In Python I made two classes: voxelSpace and detectorSpace. The voxelSpace class handles the discretization of 3D space, which I call computational space, and places a sphere with some offset to the rotation axis in the computational space. In practice it associates the number 0 for each voxel outside the sphere and 1 for each voxel inside the sphere. The detectorSpace class handles the detector geometry and all functions needed to calculate the intersection of the cone with the sphere. I calculated the intersection the same as the method presented in Ref. [2].\nThe first program prototype was fantastically slow and would not yield anything. Some rewriting and parallelizing the code with the help of Ref. [3,4] made it much faster but still incredibly slow. I thought it was the rotation that took the longest so I wrote a separate rotation method based on Ref. [5]. In the end I could produce figure (2).\nFigure 2: Left - The phantom sliced in the middle. Middle - The ordinary Radon projection. Right - The cone-surface projections.\nConclusion Well, the generated cone-surface projection in figure 2 look kind of similar to the ones in figure 1, if I squint. The image in figure 1 was produced for a very rough voxel grid and very few detectors. It took around 20 seconds to get the image in figure 2. The Python code that I wrote scales very poorly. Increasing the resolution of the voxel grid to twice the resolution in figure 2 takes approximately four times as long to calculate.\nI think the main bottleneck is the indicator function in the detectorSpace class, which is based on the one in Ref. [2]. It check each voxel that is touched by the sphere and determines if that voxel also is contained on the surface of the sphere. I do not know how to do it more efficiently at this moment but I think this is where one should look for improvements first.\nThe code is available on my Github account\nReferences [1] Application of spherical harmonics to image reconstruction for the Compton camera, Basko, Roman; Zeng, Gengsheng; Gullberg, Grant T, Link\n[2] A model of spatial resolution uncertainty for Compton camera imaging, Yanting Ma , Joshua Rapp, Petros Boufounos, Hassan Mansour, Link\n[3] Parallel programming in Python - multiprocessing (Pool and apply_async methods), Xin Li, Link\n[4] Parallel programming in Python - multiprocessing (Process and Queue), Xin Li, Link\n[5] Rotation of Voxels in 3D Space using Python, Kok Wei Chew, Link\n","permalink":"http://localhost:1313/posts/cone-surface-projection-in-practice/","summary":"\u003cp\u003e\u003cstrong\u003eOne of the first properties encountered in Compton camera image reconstruction is the cone-surface projection. Understandig the properties of this geometrical object will set the ground works for understanding more advanced models.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI was reading some articles of the early versions of the Compton camera image reconstruction procedure when I came across Ref.[1] talking about the \u003cem\u003econe-surface projections\u003c/em\u003e. They investigated a primitive version of a \u0026lsquo;\u0026lsquo;Compton camera\u0026rsquo;\u0026rsquo;. Well, it kind of had some properties like a Compton camera like a scatterer and an absorber but they assumed that photons only scatter at a fixed given angle. This camera measures the cone-surface projection of a source. What this means is that the intensity measured at a given detector is proportional to the integral over the intersection of the cone and the source, given that the source is homogeneous.Â  Specifically, let $f(\\mathbf{x})$ denote the source distribution, $\\mathbf{k}$ the cone axis making angle $\\beta$ with $\\mathbf{n}$ along the cone surface, they defined\u003c/p\u003e","title":"Cone-Surface Projections"}]