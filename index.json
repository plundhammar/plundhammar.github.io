[{"content":"Delving into the practicalities of the inverse problem of reconstructing Compton camera images will lead you to Legandre polynomials or Maximum Likelihood methods, most likely.\nSince last week I have been interested in both the analytical reconstruction techniques as well as some of the iterative methods.\nRegarding iterative methods, what I found is that it is difficult to get a feeling on how the iterative methods properly work simply because the algorithms (I write mine in Python) is incredibly slow. However, I have made some progress and have been able to reproduce some results but in a very corse voxel space. One way of simplifying things is to simplify the system matrix. I was reading Ref. [1] where they sort of reduce the forward problem to the following statement:\nThe probability of observing a given measurement $$ A_i = [E_0,E\u0026rsquo;,r_{01},r_{12}] $$ given a gamma ray incident from pixel $j$ is $$ t_{ij} = \\exp(-\\sigma_t(E_0)r_{01})\\frac{\\mathrm{d}\\sigma_C}{\\mathrm{d}\\Omega}\\exp(-\\sigma_t(E\u0026rsquo;)r_{12}), $$ where $\\sigma_t(E)$ is the total absorption cross section at energy $E$, $E_0$ and $E\u0026rsquo;$ the initial and scattered gamma-ray energies, respectively, $r_{01}$ the attenuation distance between the first and second interactions, and $\\mathrm{d}\\sigma_C/\\mathrm{d}\\Omega$ the Compton cross-section divided by $r_{12}^2$.\nBasically you could write a function calculating the values $\\sigma_t(E)$ and $\\mathrm{d}\\sigma_C/\\mathrm{d}\\Omega$ and the distances on the fly without storing the values in huge matrices. I think this approach could be fruitful.\nThe computational complexity of the list-mode likelihood methods are a huge issue still. However, being clever like in Refs. [2,3], one can speed up these types of algorithms by a factor of 250(!) by running it on 8 GPU units.\nAn interesting counterpart to the iterative modeling is the analytical methods. I read Ref.[4], where they derive all necessary properties , like the point spread function and deconvolution kernel, to be able to fully (at least in theory disregarding numerical issues) reconstruct any image obtained from Compton camera event data. They start with the following statement:\nThe probability distribution $p(\\omega)$ of measuring an event with scatter angle $\\omega$ is proportional to the differential cross-section, $h(\\cos\\omega)$.\nNote that we are ignoring spatial and energy resolution of the detector as well as absorption probabilities within the detector. This model is a perfect study to understand the mathematical issues arising from the geometry of the Compton camera. They conclude, like in Ref. [5], that simply applying a spherical deconvolution will not work. Specifically, consider the model $$ f(\\Omega_2) = \\int\\mathrm{d}\\Omega_1 g(\\Omega_1)h(\\cos\\omega) $$ where $f(\\Omega_2)$ is the measured image intensity summed over all measured scatter angles and using the statement above. One might be tempted to try to invert this equation directly, and it is actually possible! They show in the Appendix that there exists an $h^{-1}(\\cos\\omega)$ such that $$ g(\\Omega_1) = \\int \\mathrm{d}\\Omega_2 f(\\Omega_2)h^{-1}(\\cos\\omega) $$ where $$ h^{-1}(\\cos\\omega) = \\sum_{n=0}^\\infty \\left(\\frac{2n+1}{4\\pi}\\right)\\frac{P_n(\\cos\\omega)}{H_n} $$ and $$ H_n = \\frac{2n+1}{2}\\int\\mathrm{d}(\\cos\\omega)h(\\cos\\omega)P_n(\\cos\\omega) $$ and the basis function $P_n(\\cos\\omega)$ are Legendre polynomials. However, this solution is not stable since the coefficient $H_n$ approaches zero, and since they are in the denominator in the expression for $h^{-1}(\\cos\\omega)$ this function is not stable. What they do is to define the summation image $g\u0026rsquo;(\\Omega\u0026rsquo;_1)$ which they prove can be expressed by an angular convolution of the line projections with an appropriate point spread function. Then one can reconstruct $g(\\Omega_1)$ from $g\u0026rsquo;(\\Omega_1\u0026rsquo;)$. The point spread function, called $h_{bp}(\\cos\\omega)$ is then derived and the result is $$ h_{bp}(\\cos\\omega) = \\frac{1}{\\sqrt{1-\\cos^2\\omega/2}}\\int_{-\\cos\\omega/2}^{\\cos\\omega/2}\\mathrm{d}z\\frac{h(z)}{\\sqrt{\\cos^2\\omega/2-z^2}}, $$ with $$ h(\\cos\\omega) = h_c(\\cos\\omega)\\frac{1+\\cos^2\\omega+\\frac{\\gamma^2(1-\\cos\\omega)^2}{1+\\gamma(1-\\cos\\omega)}}{(1+\\gamma(1-\\cos\\omega))^2} $$ and $h_c(\\cos\\omega)$ being the Klein-Nishina cross-section, see figure 1. The angle $\\omega$ in the expression above is the angle from a $z$-axis between the axis of the source point and the axis of the cone.\nFigure 1: $h_c(\\cos\\omega)$\nThe differential cross section convolution kernel $h(\\cos\\omega)$ can be seen in figure 2.\nFigure 2: $h(\\cos\\omega)$\nThey said that the integral above has an analytic solution but when I tried to calculate it in scipy it did not result in anything. I did manage to replicate the resulting point spread function, see figure 3.\nFigure 3: $h_bp(\\cos\\omega)$\nIt would be interesting to expand this in Legendre polynomials and deconvolve the measured image intensity summed over scattering angles and compare with an iterative method like LM-MLEM. I suppose the system matrix would take the simple form shown in Ref. [1], if I manage to write efficient code.\nConclusion Iterative methods are probably the best but they are very computationally heavy. For a toy model, one could try to simplify the system matrix to just a few calculations for each element.\nAnalytical methods are simple in their assumptions but can easily be expanded upon. Also, they are fast and efficient. It seems that the best approach is to expand some transformed version of the point spread function in Legendre polynomials, such that the expansion coefficients are far from zero. Then deconvolve the source response with the expression of the inverse transformed point spread function.\nThe code is available on my Github account\nReferences [1] 4$\\pi$ Compton Imaging Using 3-D Position-Sensitive CdZnTe Detector Via Weighted List-Mode Maximum Likelihood, Lehner, C.E; He, Zhong; Zhang, Feng Link\n[2]Maximum Likelihood Event Estimation and List-mode Image Reconstruction on GPU Hardware, Caucci, Luca; Furenlid, Lars R.; Barrett, Harrison H. Link\n[3] List-mode MLEM Image Reconstruction from 3D ML Position Estimates, Caucci, Luca; Furenlid, Lars R.; Barrett, Harrison H. Link\n[4] Reconstruction of cone-beam projections from Compton scattered data, Parra L.C Link\n[5] Application of spherical harmonics to image reconstruction for the Compton camera, Basko, Roman; Zeng, Gengsheng L; Gullberg, Grant T Link\n","permalink":"http://localhost:1313/posts/deconvolution_and_the_system_matrix/","summary":"Delving into the practicalities of the inverse problem of reconstructing Compton camera images will lead you to Legandre polynomials or Maximum Likelihood methods, most likely.\nSince last week I have been interested in both the analytical reconstruction techniques as well as some of the iterative methods.\nRegarding iterative methods, what I found is that it is difficult to get a feeling on how the iterative methods properly work simply because the algorithms (I write mine in Python) is incredibly slow.","title":"Deconvolution and the System Matrix"},{"content":"One of the first properties encountered in Compton camera image reconstruction is the cone-surface projection. Understandig the properties of this geometrical object will set the ground works for understanding more advanced models.\nI was reading some articles of the early versions of the Compton camera image reconstruction procedure when I came across Ref.[1] talking about the cone-surface projections. They investigated a primitive version of a \u0026lsquo;\u0026lsquo;Compton camera\u0026rsquo;\u0026rsquo;. Well, it kind of had some properties like a Compton camera like a scatterer and an absorber but they assumed that photons only scatter at a fixed given angle. This camera measures the cone-surface projection of a source. What this means is that the intensity measured at a given detector is proportional to the integral over the intersection of the cone and the source, given that the source is homogeneous. Specifically, let $f(\\mathbf{x})$ denote the source distribution, $\\mathbf{k}$ the cone axis making angle $\\beta$ with $\\mathbf{n}$ along the cone surface, they defined\n$$ p(\\mathbf{n}) = \\int_0^\\infty f(O+\\mathbf{n}r)r\\mathrm{d}r~~~~~~q_k(\\beta) = \\int_{S(\\mathbf{k},\\beta)}p(\\mathbf{n})\\mathrm{d}s. $$ Here $S(\\mathbf{k},\\beta)$ denotes the circle created by intersecting the cone with axis $\\mathbf{k}$ and half-angle $\\beta$ with the unit sphere, and $O$ the apex of the cone, describing the first interaction point with the scatterer. The quantity $q_{k}(\\beta)$ is the cone-surface projection. It is reasonable that the cone-surface projections are at least proportional to the photon flux at the detector for a given scattering angle and cone axis.\nNow, they showed a method to invert these equations to obtain the original source $f(\\mathbf{x})$ via Legendre transforms. However, they also showed a figure in which these cone-surface projections were shown (figure 1). I wanted to try to recreate this figure.\nFigure 1: The first two steps of image reconstruction. (a) Plane projections calculated directly from the phantom (b) Cone-surface projections (c) Plane projections evaluated by the algorithm from cone-surface integrals.\nThe cone-surface projections in (b) are calculated for scattering angle $\\beta = \\pi/4$, but they do not entail what cone axis is used or how it is chosen or how the sphere phantom is placed in the space.\nProblem Statement I wanted to generate figure 1 (b), and what I can gather from the article we have:\nA spherical phantom with a radius of 10 mm offset some distance from the rotation axis (not given but looks like around 20 mm from figure 1 (a)); Since they do not talk about specific detector elements I assume that they use all voxels along an axis in the plane perpendicular to the rotation axis. I will however define a set of detector elements along this axis; The cone-surface projection is then the total contribution of a cone with half-angle $\\beta = \\pi/4$ with some axis $\\mathbf{k}$ (not explicitly given but I will take the one perpendicular to the rotation axis) with the spherical source. Practical Issues and Result In Python I made two classes: voxelSpace and detectorSpace. The voxelSpace class handles the discretization of 3D space, which I call computational space, and places a sphere with some offset to the rotation axis in the computational space. In practice it associates the number 0 for each voxel outside the sphere and 1 for each voxel inside the sphere. The detectorSpace class handles the detector geometry and all functions needed to calculate the intersection of the cone with the sphere. I calculated the intersection the same as the method presented in Ref. [2].\nThe first program prototype was fantastically slow and would not yield anything. Some rewriting and parallelizing the code with the help of Ref. [3,4] made it much faster but still incredibly slow. I thought it was the rotation that took the longest so I wrote a separate rotation method based on Ref. [5]. In the end I could produce figure (2).\nConclusion Well, the generated cone-surface projection in figure 2 look kind of similar to the ones in figure 1, if I squint. The image in figure 1 was produced for a very rough voxel grid and very few detectors. It took around 20 seconds to get the image in figure 2. The Python code that I wrote scales very poorly. Increasing the resolution of the voxel grid to twice the resolution in figure 2 takes approximately four times as long to calculate.\nI think the main bottleneck is the indicator function in the detectorSpace class, which is based on the one in Ref. [2]. It check each voxel that is touched by the sphere and determines if that voxel also is contained on the surface of the sphere. I do not know how to do it more efficiently at this moment but I think this is where one should look for improvements first.\nThe code is available on my Github account\nReferences [1] Application of spherical harmonics to image reconstruction for the Compton camera, Basko, Roman; Zeng, Gengsheng; Gullberg, Grant T, Link\n[2] A model of spatial resolution uncertainty for Compton camera imaging, Yanting Ma , Joshua Rapp, Petros Boufounos, Hassan Mansour, Link\n[3] Parallel programming in Python - multiprocessing (Pool and apply_async methods), Xin Li, Link\n[4] Parallel programming in Python - multiprocessing (Process and Queue), Xin Li, Link [5] Rotation of Voxels in 3D Space using Python, Kok Wei Chew, Link\n","permalink":"http://localhost:1313/posts/cone-surface_projections/","summary":"One of the first properties encountered in Compton camera image reconstruction is the cone-surface projection. Understandig the properties of this geometrical object will set the ground works for understanding more advanced models.\nI was reading some articles of the early versions of the Compton camera image reconstruction procedure when I came across Ref.[1] talking about the cone-surface projections. They investigated a primitive version of a \u0026lsquo;\u0026lsquo;Compton camera\u0026rsquo;\u0026rsquo;. Well, it kind of had some properties like a Compton camera like a scatterer and an absorber but they assumed that photons only scatter at a fixed given angle.","title":"Cone-Surface Projections"}]